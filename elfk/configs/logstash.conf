# Logstash Configuration
# Best practices for log processing and forwarding

input {
  # Beats input (Filebeat, Metricbeat, etc.)
  beats {
    port => 5044
    type => "beats"
    codec => "json"
    congestion_threshold => 40
    target_field_for_codec => "message"
  }

  # TCP input for JSON logs
  tcp {
    port => 5000
    codec => json_lines
    type => "tcp-json"
  }

  # UDP input for JSON logs (syslog, etc.)
  udp {
    port => 5000
    codec => json_lines
    type => "udp-json"
  }

  # GELF input for Docker logs
  gelf {
    port => 12201
    type => "gelf"
  }

  # HTTP input for webhooks and APIs
  http {
    port => 8080
    codec => "json"
    type => "http"
  }
}

filter {
  # Add processing timestamp
  mutate {
    add_field => { "[@metadata][processed_at]" => "%{+ISO8601}" }
  }

  # Handle different input types
  if [type] == "beats" {
    # Process Filebeat logs
    if [agent][type] == "filebeat" {
      # Extract container information from Docker metadata
      if [container] {
        mutate {
          add_field => { "container_name" => "%{[container][name]}" }
          add_field => { "container_image" => "%{[container][image][name]}" }
          add_field => { "container_id" => "%{[container][id]}" }
        }
      }

      # Parse Docker logs format
      if [message] =~ /^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+Z/ {
        grok {
          match => { "message" => "%{TIMESTAMP_ISO8601:docker_timestamp} %{GREEDYDATA:log_message}" }
        }
        if [docker_timestamp] {
          date {
            match => [ "docker_timestamp", "ISO8601" ]
            target => "@timestamp"
          }
          mutate {
            remove_field => [ "docker_timestamp" ]
          }
        }
        mutate {
          replace => { "message" => "%{log_message}" }
          remove_field => [ "log_message" ]
        }
      }
    }
  }

  # Parse JSON messages
  if [message] =~ /^\s*\{.*\}\s*$/ {
    json {
      source => "message"
      target => "parsed_json"
      skip_on_invalid_json => true
    }
    
    # If JSON parsing successful, merge fields
    if [parsed_json] {
      ruby {
        code => "
          parsed = event.get('parsed_json')
          if parsed.is_a?(Hash)
            parsed.each { |k, v| event.set(k, v) unless k == 'message' }
          end
        "
      }
      mutate {
        remove_field => [ "parsed_json" ]
      }
    }
  }

  # Normalize log levels
  if [level] {
    mutate {
      uppercase => [ "level" ]
    }
    
    # Map common log level variations
    if [level] in [ "WARN", "WARNING" ] {
      mutate { replace => { "level" => "WARN" } }
    }
    if [level] in [ "ERR", "ERROR", "FATAL" ] {
      mutate { replace => { "level" => "ERROR" } }
    }
    if [level] in [ "DEBUG", "TRACE" ] {
      mutate { replace => { "level" => "DEBUG" } }
    }
  }

  # Parse common timestamp formats
  if [timestamp] {
    date {
      match => [ 
        "timestamp", 
        "ISO8601",
        "yyyy-MM-dd HH:mm:ss",
        "yyyy-MM-dd'T'HH:mm:ss.SSSZ",
        "MMM dd HH:mm:ss",
        "MMM  d HH:mm:ss"
      ]
      target => "@timestamp"
    }
  }

  # Add environment and service tags
  mutate {
    add_field => { "environment" => "docker" }
    add_field => { "pipeline" => "logstash" }
  }

  # Extract service name from container name or image
  if [container_name] {
    grok {
      match => { "container_name" => "(?<service_name>[^_-]+)" }
      tag_on_failure => ["_grokparsefailure_service_name"]
    }
  } else if [container_image] {
    grok {
      match => { "container_image" => "(?<service_name>[^/:]+)" }
      tag_on_failure => ["_grokparsefailure_service_name"]
    }
  }

  # Parse stack traces for Java/Python applications
  if [message] =~ /Exception|Error|Traceback/ {
    mutate {
      add_tag => [ "exception", "error" ]
    }
  }

  # Parse HTTP access logs
  if [message] =~ /^\d+\.\d+\.\d+\.\d+/ {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
      tag_on_failure => ["_grokparsefailure_apache"]
    }
  }

  # Clean up unnecessary fields
  mutate {
    remove_field => [ 
      "[agent][ephemeral_id]",
      "[agent][id]",
      "[ecs][version]",
      "[input][type]",
      "[log][file][path]"
    ]
  }

  # Add GeoIP information for IP addresses
  if [clientip] {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }
}

output {
  # Primary output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    
    # Dynamic index naming based on service and date
    index => "%{[service_name]:logstash}-%{+YYYY.MM.dd}"
    
    # Template settings
    template_name => "logstash"
    template_pattern => "logstash-*"
    template_overwrite => true
    
    # Performance settings
    workers => 2
    flush_size => 500
    idle_flush_time => 1
    
    # Retry settings
    retry_max_interval => 5
    retry_max_times => 5
    
    # Document settings
    document_type => "_doc"
    
    # Failure handling
    action => "index"
  }

  # Error handling - send failed documents to a separate index
  if "_jsonparsefailure" in [tags] or "_grokparsefailure" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "logstash-errors-%{+YYYY.MM.dd}"
      document_type => "_doc"
    }
  }

  # Debug output (enable for troubleshooting)
  # stdout { 
  #   codec => rubydebug {
  #     metadata => true
  #   }
  # }

  # Optional: Send metrics to monitoring
  # statsd {
  #   host => "statsd"
  #   port => 8125
  #   namespace => "logstash"
  #   sender => "%{host}"
  #   gauge => [ "processing_time", "%{[@metadata][processing_time]}" ]
  # }
}
